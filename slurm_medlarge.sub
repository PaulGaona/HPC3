{\rtf1\ansi\ansicpg1252\cocoartf2821
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 #!/bin/bash\
\
#SBATCH --job-name=medlarge.cov.medlarge.n    ## Name of the job.\
#SBATCH -A AKENNEY1_LAB      ## account names (AKENNEY1_LAB)\
\
#SBATCH --partition=standard               ## partition name (free, stats.p,\
#SBATCH --nodes=1             ## (-N) number of nodes to use\
#SBATCH --ntasks=40          ## (# Fill in how many cpu cores you'd like)\
#SBATCH --cpus-per-task=1     ## number of cores the job needs\
\
#SBATCH --mem-per-cpu=6G    # requesting max memory per CPU\
#SBATCH --time=96:00:00 # Fill in the max run time to allow (hr:min:sec)\
\
#SBATCH --mail-type=end,fail ## what will be emailed\
#SBATCH --mail-user=pgaonapa@uci.edu ## email notification\
\
#SBATCH --error=slurm-%J.err  ## error log file\
#SBATCH --output=slurm-%J.out ## output log file\
\
module load R/4.3.3\
Rscript scripts_medlarge.R\
}